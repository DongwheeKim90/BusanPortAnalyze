import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import folium
from folium.features import CustomIcon
from streamlit_folium import st_folium
import json

st.set_page_config(layout="wide")

st.title("Vacancy data analysis for ship supplies storage", anchor=False)

tab_1, tab_2, tab_3 = st.tabs(["Analysis Process(EN)", "Analysis Process(KR)", "Data Preprocessing"])

with tab_1:
    st.markdown("""
    Previously, we conducted an analysis covering <span style='color:orange; font-weight:bold; font-size:20px;'>correlation with potential customers, product selection based on annual ship supply demand, and the identification of companies dealing with those products</span> for building an online ship supply platform.
    In this section, we performed a <span style='color:white; font-weight:bold; font-size:20px;'>visual analysis of vacancy data</span>.
    But why did we collect and analyze vacancy data in the first place?
    We assumed that if an online ship supply platform is launched, it would be important for foreign crew members to be able to <span style='color:white; font-weight:bold; font-size:20px;'>pick up their ordered products near the port during embarkation or disembarkation</span>.
    This would help <span style='color:orange; font-weight:bold; font-size:20px;'>crew members plan their time more efficiently and make the most of their stay</span>.
    Based on this rationale, we collected and visualized vacancy data around <span style='color:orange; font-weight:bold; font-size:20px;'>the three major ports of Busan (New Port, North Port, and Gamcheon Port)</span>.
    <br><span style='color:red; font-weight:bold; font-size:15px;'>(* Due to confidentiality policies of real estate agencies, detailed vacancy addresses and building names were not publicly available. Therefore, we gathered, processed, and visualized data by searching areas near the three major port addresses.)</span>
    """, unsafe_allow_html=True)

    vacancyData = pd.read_csv("./useData/finishPrepro/vacancy_locationLaLo.csv", encoding="utf-8-sig")
    vacancyData["FloorType"] = ["ì§€í•˜" if "B" in v else "ì§€ìƒ" for v in vacancyData["Floor"]]

   # ì˜ˆì‹œ ì „ì²˜ë¦¬ í•¨ìˆ˜
    def deleteFloor(x):
        x = x.replace(" Floor", "").replace("B","")
        return x

    vacancyData["Floor"] = vacancyData["Floor"].apply(deleteFloor)
    vacancyData["Floor"] = vacancyData["Floor"].astype("int64")

    # ì—´ ì •ë¦¬
    renameColumns = ['FloorType', 'Address', 'Size', 'Floor', 'Month Price', 'Depossit Price', 'Latitude', 'Longitude']
    vacancyData = vacancyData[renameColumns]

    # ê·¸ë£¹ë³„ ì •ë³´ ì •ë¦¬
    cntType = vacancyData.groupby(["Address", "FloorType"])["FloorType"].count()

    group_countType = pd.DataFrame({
        "Address": [v[0] for v in cntType.index],
        "FloorType": [v[1] for v in cntType.index],
        "Type": cntType.values
    })

    groupPriceMean = vacancyData.groupby("Address")[["Month Price", "Depossit Price"]].mean()
    groupPriceMeanResult = pd.DataFrame({
        "Address": groupPriceMean.index,
        "Month Price(AVG)": groupPriceMean["Month Price"].astype(int),
        "Depossit Price(AVG)": groupPriceMean["Depossit Price"].astype(int)
    }).reset_index(drop=True)

    groupSizeMean = vacancyData.groupby("Address")["Size"].mean()
    groupFloorMean = vacancyData.groupby("Address")["Floor"].mean()

    groupMerge = (
        groupPriceMeanResult
        .merge(groupFloorMean, on="Address")
        .merge(groupSizeMean, on="Address")
    )

    # ì§€ë„ ìƒì„±
    targetArea = folium.Map(
        location=[35.125135443661655, 128.96211911293543],
        zoom_start=11,
        tiles="CartoDB positron"
    )

    # ë¶€ì‚° geojson ì ìš©
    busanGeo = "./useData/koreaBusan.geojson"
    with open(busanGeo, encoding="utf-8-sig") as f:
        myGeo = json.load(f)

    def myGeo_style(x):
        return {
            "fillColor": "#dae080",
            "color": "",
            "weight": 0.5,
            "fillOpacity": 0.2
        }

    folium.GeoJson(data=myGeo, style_function=myGeo_style).add_to(targetArea)

    # ì§€ë„ì— ë§ˆì»¤ ì¶”ê°€
    for i, v in vacancyData.iterrows():
        lat = v["Latitude"]
        lon = v["Longitude"]
        address = v["Address"]

        if (address in groupMerge["Address"].values) and (address in group_countType["Address"].values):
            avg_floor = int(groupMerge[groupMerge["Address"] == address]["Floor"].values[0])
            avg_size = int(groupMerge[groupMerge["Address"] == address]["Size"].values[0])
            avg_month = groupMerge[groupMerge["Address"] == address]["Month Price(AVG)"].values[0]
            avg_deposit = groupMerge[groupMerge["Address"] == address]["Depossit Price(AVG)"].values[0]

            formatted_month = f"{avg_month:,}"
            formatted_deposit = f"{avg_deposit:,}"
            formatted_size = f"{avg_size:,}"

            floor_info_rows = group_countType[group_countType["Address"] == address]
            floor_type_strs = [f"{row['FloorType']} (count: {row['Type']})" for _, row in floor_info_rows.iterrows()]
            floor_type_combined = ", ".join(floor_type_strs)

            noticeInfo = (
                f"<div style='background-color:#f9f9f9; padding:10px; border-radius:8px; font-size:13px; line-height:1.6'>"
                f"<b>(1) Area Name</b>: {address}<br>"
                f"<b>(2) Vacancy Type</b>: {floor_type_combined}<br>"
                f"<b>(3) Average Floor</b>: {avg_floor}<br>"
                f"<b>(4) Average Size</b>: {formatted_size}ã¡<br>"
                f"<b>(5) Average Monthly Price (KRW)</b>: {formatted_month}<br>"
                f"<b>(6) Average Monthly Deposit (KRW)</b>: {formatted_deposit}"
                f"</div>"
            )

            popup = folium.Popup(noticeInfo, max_width=400, min_width=30, max_height=300, show=True)

            myIcon = "./useData/myImage/vacancyMan.png"
            myIcon_edit = CustomIcon(
                icon_image=myIcon,
                icon_size=(31, 31),
                icon_anchor=(15, 15)
            )

            folium.Marker(
                location=[lat, lon],
                icon=myIcon_edit
            ).add_child(popup).add_to(targetArea)

        # ì›í˜• ë§ˆì»¤ëŠ” ëª¨ë“  ê³µì‹¤ì— ì¶”ê°€
        folium.CircleMarker(
            location=[lat, lon],
            radius=60,
            fill=True,
            fill_color="#e7ede3",
            color="#ee928c",
            fill_opacity=0.1
        ).add_to(targetArea)

    # âœ… ì§€ë„ ì¶œë ¥ì€ forë¬¸ ë°”ê¹¥ì—ì„œ ë‹¨ 1ë²ˆë§Œ ì‹¤í–‰
    st_folium(targetArea, use_container_width=True, key="vacancy_en")

    st.subheader("Conclusion", anchor=False)

    st.markdown('''
    Through the above vacancy data visualization, we can <span style='color:orange; font-weight:bold; font-size:20px;'>secure optimal geographic logistics hubs near each port</span>,
    which can also serve as a valuable reference for <span style='color:orange; font-weight:bold; font-size:20px;'>budget estimation and allocation for the online ship supply platform</span>.
    ''', unsafe_allow_html=True)
    st.dataframe(groupMerge, hide_index=True)


with tab_2:
    st.markdown("""
    ì•ì„œ ìš°ë¦¬ëŠ” ì˜¨ë¼ì¸ ì„ ìš©í’ˆ ì‡¼í•‘ëª° êµ¬ì¶•ì„ ìœ„í•œ <span style='color:orange; font-weight:bold; font-size:20px;'>ì ì¬ ê³ ê°ì„ ìœ„í•œ ìƒê´€ê´€ê³„ ë¶„ì„, ì—°ë„ë³„ ì„ ìš©í’ˆ ìˆ˜ìš” ì¡°ì‚¬ë¥¼ í†µí•œ í’ˆëª© ì„ ì •, í•´ë‹¹ í’ˆëª©ì„ ì·¨ê¸‰í•˜ëŠ” ê¸°ì—…ì— ëŒ€í•œ ë¶„ì„</span>ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.
    ì´ë²ˆ í˜ì´ì§€ì—ì„œëŠ” <span style='color:white; font-weight:bold; font-size:20px;'>ê³µì‹¤ ë°ì´í„°ì— ëŒ€í•œ ì‹œê°í™” ë¶„ì„</span>ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
    ê·¸ë ‡ë‹¤ë©´, ì™œ ê³µì‹¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í–ˆì„ê¹Œìš”?
    ì˜¨ë¼ì¸ ì„ ìš©í’ˆ ì‡¼í•‘ëª°ì´ ìš´ì˜ëœë‹¤ë©´, ì™¸êµ­ ì„ ì›ë“¤ì´ <span style='color:white; font-weight:bold; font-size:20px;'>ìŠ¹ì„  ë˜ëŠ” í•˜ì„  ì‹œ í•­êµ¬ ì¸ê·¼ì—ì„œ ì œí’ˆì„ ì§ì ‘ ìˆ˜ë ¹</span>í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.
    ì´ëŠ” <span style='color:orange; font-weight:bold; font-size:20px;'>ì„ ì›ë“¤ì´ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì‹œê°„ ê³„íšì„ ì„¸ìš°ê³ , ì²´ë¥˜ ì¤‘ ë‹¤ì–‘í•œ í™œë™ì„ í•˜ëŠ” ë° ë„ì›€</span>ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì´ì— ë”°ë¼, ìš°ë¦¬ëŠ” <span style='color:orange; font-weight:bold; font-size:20px;'>ë¶€ì‚°ì˜ 3ëŒ€ í•­êµ¬(ì‹ í•­, ë¶í•­, ê°ì²œí•­)</span>ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì¸ê·¼ ì§€ì—­ì˜ ê³µì‹¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì•„ë˜ì™€ ê°™ì´ ì‹œê°í™”í•˜ì˜€ìŠµë‹ˆë‹¤.
    <br><span style='color:red; font-weight:bold; font-size:15px;'>(ë°ì´í„° ì¶œì²˜ì—ì„œ ë¶€ë™ì‚° ì¤‘ê°œì†Œì˜ ì˜ì—…ê´€ë ¨ ê¸°ë°€ë¡œ ê³µì‹¤ê´€ë ¨ ë¹Œë”©ëª…, ê³µì‹¤ ì„¸ë¶€ì£¼ì†Œë¥¼ ê³µê°œí•˜ì§€ ì•Šì•„, 3ëŒ€ í•­êµ¬ë³„ ì£¼ì†Œ ì…ë ¥ í›„ ê·¸ ì¸ê·¼ì— ê²€ìƒ‰ë˜ëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì§‘/ê°€ê³µ/ì‹œê°í™” í–ˆìŠµë‹ˆë‹¤.)</span>
    """, unsafe_allow_html=True)

    vacancyData_2 = pd.read_csv("./useData/finishPrepro/vacancy_locationLaLo.csv", encoding="utf-8-sig")
    vacancyData_2["FloorType"] = ["ì§€í•˜" if "B" in v else "ì§€ìƒ" for v in vacancyData_2["Floor"]]

   # ì˜ˆì‹œ ì „ì²˜ë¦¬ í•¨ìˆ˜
    def deleteFloor(x):
        x = x.replace(" Floor", "").replace("B","")
        return x

    vacancyData_2["Floor"] = vacancyData_2["Floor"].apply(deleteFloor)
    vacancyData_2["Floor"] = vacancyData_2["Floor"].astype("int64")

    # ì—´ ì •ë¦¬
    renameColumns = ['FloorType', 'Address', 'Size', 'Floor', 'Month Price', 'Depossit Price', 'Latitude', 'Longitude']
    vacancyData_2 = vacancyData_2[renameColumns]

    # ê·¸ë£¹ë³„ ì •ë³´ ì •ë¦¬
    cntType_2 = vacancyData_2.groupby(["Address", "FloorType"])["FloorType"].count()

    group_countType_2 = pd.DataFrame({
        "Address": [v[0] for v in cntType_2.index],
        "FloorType": [v[1] for v in cntType_2.index],
        "Type": cntType_2.values
    })

    groupPriceMean_2 = vacancyData_2.groupby("Address")[["Month Price", "Depossit Price"]].mean()
    groupPriceMeanResult_2 = pd.DataFrame({
        "Address": groupPriceMean_2.index,
        "Month Price(AVG)": groupPriceMean_2["Month Price"].astype(int),
        "Depossit Price(AVG)": groupPriceMean_2["Depossit Price"].astype(int)
    }).reset_index(drop=True)

    groupSizeMean_2 = vacancyData_2.groupby("Address")["Size"].mean()
    groupFloorMean_2 = vacancyData_2.groupby("Address")["Floor"].mean()

    groupMerge_2 = (
        groupPriceMeanResult
        .merge(groupFloorMean, on="Address")
        .merge(groupSizeMean, on="Address")
    )

    # ì§€ë„ ìƒì„±
    targetArea_2 = folium.Map(
        location=[35.125135443661655, 128.96211911293543],
        zoom_start=11,
        tiles="CartoDB positron"
    )

    # ë¶€ì‚° geojson ì ìš©
    busanGeo_2 = "./useData/koreaBusan.geojson"
    with open(busanGeo_2, encoding="utf-8-sig") as f:
        myGeo = json.load(f)

    def myGeo_style(x):
        return {
            "fillColor": "#dae080",
            "color": "",
            "weight": 0.5,
            "fillOpacity": 0.2
        }

    folium.GeoJson(data=myGeo, style_function=myGeo_style).add_to(targetArea_2)

    # ì§€ë„ì— ë§ˆì»¤ ì¶”ê°€
    for i, v in vacancyData.iterrows():
        lat = v["Latitude"]
        lon = v["Longitude"]
        address = v["Address"]

        if (address in groupMerge["Address"].values) and (address in group_countType["Address"].values):
            avg_floor = int(groupMerge[groupMerge["Address"] == address]["Floor"].values[0])
            avg_size = int(groupMerge[groupMerge["Address"] == address]["Size"].values[0])
            avg_month = groupMerge[groupMerge["Address"] == address]["Month Price(AVG)"].values[0]
            avg_deposit = groupMerge[groupMerge["Address"] == address]["Depossit Price(AVG)"].values[0]

            formatted_month = f"{avg_month:,}"
            formatted_deposit = f"{avg_deposit:,}"
            formatted_size = f"{avg_size:,}"

            floor_info_rows = group_countType[group_countType["Address"] == address]
            floor_type_strs = [f"{row['FloorType']} (count: {row['Type']})" for _, row in floor_info_rows.iterrows()]
            floor_type_combined = ", ".join(floor_type_strs)

            noticeInfo = (
                f"<div style='background-color:#f9f9f9; padding:10px; border-radius:8px; font-size:13px; line-height:1.6'>"
                f"<b>(1) Area Name</b>: {address}<br>"
                f"<b>(2) Vacancy Type</b>: {floor_type_combined}<br>"
                f"<b>(3) Average Floor</b>: {avg_floor}<br>"
                f"<b>(4) Average Size</b>: {formatted_size}ã¡<br>"
                f"<b>(5) Average Monthly Price (KRW)</b>: {formatted_month}<br>"
                f"<b>(6) Average Monthly Deposit (KRW)</b>: {formatted_deposit}"
                f"</div>"
            )

            popup_2 = folium.Popup(noticeInfo, max_width=400, min_width=30, max_height=300, show=True)

            myIcon_2 = "./useData/myImage/vacancyMan.png"
            myIcon_edit_2 = CustomIcon(
                icon_image=myIcon_2,
                icon_size=(31, 31),
                icon_anchor=(15, 15)
            )

            folium.Marker(
                location=[lat, lon],
                icon=myIcon_edit_2
            ).add_child(popup_2).add_to(targetArea_2)

        # ì›í˜• ë§ˆì»¤ëŠ” ëª¨ë“  ê³µì‹¤ì— ì¶”ê°€
        folium.CircleMarker(
            location=[lat, lon],
            radius=60,
            fill=True,
            fill_color="#e7ede3",
            color="#ee928c",
            fill_opacity=0.1
        ).add_to(targetArea_2)

    # âœ… ì§€ë„ ì¶œë ¥ì€ forë¬¸ ë°”ê¹¥ì—ì„œ ë‹¨ 1ë²ˆë§Œ ì‹¤í–‰
    st_folium(targetArea_2, use_container_width=True, key="vacancy_kr")

    st.subheader("ê²°ë¡ ", anchor=False)
    st.markdown('''
        ìœ„ì˜ ê³µì‹¤ ì‹œê°í™” ë°ì´í„°ë¥¼ í†µí•˜ì—¬ <span style='color:orange; font-weight:bold; font-size:20px;'>í•­êµ¬ë³„ ìµœì ì˜ ì§€ë¦¬ì  ìœ í†µ ê±°ì ì„ ì„ ì í•  ìˆ˜ ìˆìœ¼ë©°, ì˜¨ë¼ì¸ ì„ ìš©í’ˆ ì‡¼í•‘ëª° ì˜ˆì‚° ì‚°ì • ë° ë°°ë¶„ì— ì°¸ê³ </span> í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ''', unsafe_allow_html=True)
with tab_3:
    st.markdown('''
    (1) Data Source : "https://new.land.naver.com/offices?ms=35.087642,128.8114855,16&a=SG&e=RETAIL&ad=true", "https://new.land.naver.com/offices?ms=35.094507,128.835046,16&a=SG&e=RETAIL&ad=true", "https://new.land.naver.com/offices?ms=35.120913,129.0412781,17&a=SG&e=RETAIL&ad=true", "https://new.land.naver.com/offices?ms=35.081925,128.987775,16&a=SG&e=RETAIL&ad=true" <br>
    (2) Collected Data : To collect vacancy data of warehouses used for storing ship stores and supplies.(ì„ ìš©í’ˆ ë° ì„ ë°•ìš© ë¬¼í’ˆ ë³´ê´€ì„ ìœ„í•œ ì°½ê³ ì˜ ê³µì‹¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•¨.)<br>
    (3) Data Type : Structured Data(ì •í˜• ë°ì´í„°)<br>
    (4) Technologies Used : Selenium, BeautifulSoup(bs4), Pandas<br>
    (5) Data Collection and Preprocessing Process
    ''',unsafe_allow_html=True)
    st.markdown("<span style='color:orange; font-weight:bold; font-size:20px;'>Data Collection (ë°ì´í„° ìˆ˜ì§‘)</span>", unsafe_allow_html=True)
    st.video("./useData/vacancyData.mp4")
    st.code('''
    # Author: DongWhee Kim
    # Date: 2025-04-20
    # Description: Collect vacancy data of warehouses used for ship stores using Selenium and BeautifulSoup. (ì„ ìš©í’ˆ ë³´ê´€ ì°½ê³  ê³µì‹¤ ë°ì´í„° ìˆ˜ì§‘)

    from bs4 import BeautifulSoup  # HTML parsing (HTML íŒŒì‹±ì„ ìœ„í•œ BeautifulSoup)
    import pandas as pd  # Data processing (ë°ì´í„° ì²˜ë¦¬ìš© pandas)
    import requests as req  # HTTP requests (ì›¹ ìš”ì²­ì„ ìœ„í•œ requests)
    from selenium import webdriver  # Selenium WebDriver ëª¨ë“ˆ ì„í¬íŠ¸
    from selenium.webdriver.chrome.service import Service  # Chrome ë“œë¼ì´ë²„ ì„œë¹„ìŠ¤ ì„¤ì •
    from selenium.webdriver.common.by import By  # ìš”ì†Œ íƒìƒ‰ì„ ìœ„í•œ By í´ë˜ìŠ¤
    from selenium.webdriver.support.ui import WebDriverWait  # ëª…ì‹œì  ëŒ€ê¸° ì„¤ì •ìš© WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC  # ì¡°ê±´ ì¶©ì¡± ëŒ€ê¸°ìš© EC í´ë˜ìŠ¤
    from selenium.common.exceptions import ElementClickInterceptedException, StaleElementReferenceException  # ì˜ˆì™¸ ì²˜ë¦¬ í´ë˜ìŠ¤
    from selenium.webdriver.common.keys import Keys  # í‚¤ë³´ë“œ ì´ë²¤íŠ¸ ì…ë ¥ìš© Keys
    from selenium.webdriver import ActionChains  # ë§ˆìš°ìŠ¤ ë™ì‘ ìë™í™”ë¥¼ ìœ„í•œ ActionChains
    import time  # ì‹œê°„ ì§€ì—°ìš© time ëª¨ë“ˆ
    import re  # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš© ëª¨ë“ˆ

    # Set Chrome browser options (Chrome ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì •)
    optionSet = webdriver.ChromeOptions()
    optionSet.add_argument("no-sandbox")  # Disable sandbox mode for compatibility (ìƒŒë“œë°•ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™”)

    # Start Chrome browser with specified service and options (ì„œë¹„ìŠ¤ ë° ì˜µì…˜ ì„¤ì •í•˜ì—¬ Chrome ì‹¤í–‰)
    myChrome = Service("../autoDriver/chromedriver.exe")
    myChrome = webdriver.Chrome(service=myChrome, options=optionSet)
    action = ActionChains(myChrome)  # ActionChains ê°ì²´ ì´ˆê¸°í™”

    myChrome.maximize_window()  # Maximize browser window (ë¸Œë¼ìš°ì € ì°½ ìµœëŒ€í™”)
    waitTime = WebDriverWait(myChrome, 3)  # Set explicit wait to 3 seconds (3ì´ˆ ëª…ì‹œì  ëŒ€ê¸°)

    # Define helper functions for element selection (ìš”ì†Œ ì„ íƒ í•¨ìˆ˜ ì •ì˜)
    def bs4_find(value):
        return mysoup.select_one(value)

    def bs4_finds(value):
        return mysoup.select(value)

    def selenium_find(value):
        return waitTime.until(EC.presence_of_element_located((By.CSS_SELECTOR, value)))

    def selenium_finds(value):
        return waitTime.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, value)))

    def safe_get_text(x):  # None ë°©ì§€ìš© ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
        if x is None:
            return "-"
        else:
            return x

    # Target warehouse listing URLs (í¬ë¡¤ë§ ëŒ€ìƒ URL ë¦¬ìŠ¤íŠ¸)
    link_url = [
        "offices?ms=35.087642,128.8114855,16&a=SG&e=RETAIL&ad=true",
        "offices?ms=35.094507,128.835046,16&a=SG&e=RETAIL&ad=true",
        "offices?ms=35.120913,129.0412781,17&a=SG&e=RETAIL&ad=true",
        "offices?ms=35.081925,128.987775,16&a=SG&e=RETAIL&ad=true"
    ]

    # Lists to store collected data (ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸)
    vacancy_address_list = list()
    vacancy_size_list = list()
    vacancy_floor_list = list()
    depositPrice_list = list()
    monthlyPrice_list = list()

    click_count = 0  # Click counter (í´ë¦­ íšŸìˆ˜ ì¹´ìš´í„°)

    # Iterate over all warehouse map URLs (URL ë³„ë¡œ ë°˜ë³µ ì ‘ê·¼)
    for m in range(len(link_url)):
        myChrome.get("https://new.land.naver.com/" + link_url[m])  # Visit target URL (ëŒ€ìƒ URL ì ‘ì†)
        time.sleep(5)
        print(f"[{m+1}/{len(link_url)}] ì ‘ì† ì™„ë£Œ : {link_url[m]}")

        click_list = selenium_finds("div.item.false")  # Item list fetch (ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°)

        if m < 1:  # ì²« í˜ì´ì§€ì˜ ê²½ìš°
            for n in range(len(click_list)):
                try:
                    myChrome.execute_script("arguments[0].scrollIntoView(true);", click_list[n])  # Scroll to item (í•­ëª©ê¹Œì§€ ìŠ¤í¬ë¡¤)
                    time.sleep(0.5)
                    click_list[n].click()  # Click item (ë§¤ë¬¼ í´ë¦­)
                    time.sleep(3)
                    click_count += 1

                    currentPage = myChrome.page_source  # Get page source (í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥)
                    url = myChrome.current_url

                    headersSet = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
                    }
                    res = req.get(url, headers=headersSet)
                    mysoup = BeautifulSoup(currentPage, "html.parser")  # Parse with BeautifulSoup (ë·°í‹°í’€ìˆ² íŒŒì‹±)

                    if "ì›”ì„¸" in bs4_find("div.info_article_price").get_text():  # If it's monthly rent (ì›”ì„¸ì¼ ê²½ìš°)
                        core_area_1 = bs4_find("div.info_article_price > span.price")
                        depositPrice = safe_get_text(core_area_1.get_text(strip=True).split("/")[0])
                        depositPrice_list.append(depositPrice)
                        monthlyPrice = safe_get_text(core_area_1.get_text(strip=True).split("/")[1])
                        monthlyPrice_list.append(monthlyPrice)

                        core_area_2 = bs4_finds("div.detail_box--summary > table.info_table_wrap > tbody > tr.info_table_item > td.table_td")
                        vacancy_address = safe_get_text(core_area_2[0].get_text(strip=True))
                        vacancy_address_list.append(vacancy_address)
                        vacancy_size = safe_get_text(core_area_2[2].get_text(strip=True).split("/")[1].split("ã¡")[0])
                        vacancy_size_list.append(vacancy_size)
                        vacancy_floor = safe_get_text(core_area_2[3].get_text(strip=True).split("/")[0] + " Floor")
                        vacancy_floor_list.append(vacancy_floor)
                        print(f"{click_count} ë²ˆì§¸ ë§¤ë¬¼")
                        time.sleep(3)

                    print(f"{click_count} ë²ˆì§¸ ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ")
                    print(f"ğŸ“ ì£¼ì†Œ: {vacancy_address}")
                    print(f"ğŸ“ ë©´ì : {vacancy_size}í‰")
                    print(f"ğŸ¢ ì¸µìˆ˜: {vacancy_floor}")
                    print(f"ğŸ’° ë³´ì¦ê¸ˆ: {depositPrice}, ì›”ì„¸: {monthlyPrice}")

                except ElementClickInterceptedException:
                    print(f"{n} ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì‹¤íŒ¨: ë‹¤ë¥¸ ìš”ì†Œì— ê°€ë ¤ì ¸ ìˆìŒ")
                except StaleElementReferenceException:
                    print(f"{n} ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì‹¤íŒ¨: ìš”ì†Œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ (stale)")
                except Exception as e:
                    print(f"{n} ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜ˆì™¸ ë°œìƒ: {e}")

        else:  # ì²« í˜ì´ì§€ê°€ ì•„ë‹ ê²½ìš°
            myChrome.execute_script(f"window.open('{link_url[m]}');")  # ìƒˆ íƒ­ ì—´ê¸°
            original_tab = myChrome.current_window_handle  # í˜„ì¬ íƒ­ ì €ì¥
            time.sleep(5)

            WebDriverWait(myChrome, 10).until(lambda d: len(d.window_handles) > 1)
            all_tabs = myChrome.window_handles
            for tab in all_tabs:
                if tab != original_tab:
                    myChrome.switch_to.window(tab)
                    break

            time.sleep(5)  # íƒ­ ë¡œë”© ëŒ€ê¸°

            click_list = selenium_finds("div.item.false")  # ìƒˆ íƒ­ì—ì„œ ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ì¬íƒìƒ‰

            for n in range(len(click_list)):
                try:
                    currentPage = myChrome.page_source
                    url = myChrome.current_url

                    headersSet = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
                    }
                    res = req.get(url, headers=headersSet)
                    mysoup = BeautifulSoup(currentPage, "html.parser")

                    myChrome.execute_script("arguments[0].scrollIntoView(true);", click_list[n])
                    time.sleep(0.5)
                    click_list[n].click()
                    time.sleep(4)
                    click_count += 1

                    currentPage = myChrome.page_source
                    url = myChrome.current_url
                    res = req.get(url, headers=headersSet)
                    mysoup = BeautifulSoup(currentPage, "html.parser")

                    if "ì›”ì„¸" in bs4_find("div.info_article_price").get_text():
                        core_area_1 = bs4_find("div.info_article_price > span.price")
                        depositPrice = safe_get_text(core_area_1.get_text(strip=True).split("/")[0])
                        depositPrice_list.append(depositPrice)
                        monthlyPrice = safe_get_text(core_area_1.get_text(strip=True).split("/")[1])
                        monthlyPrice_list.append(monthlyPrice)

                        core_area_2 = bs4_finds("div.detail_box--summary > table.info_table_wrap > tbody > tr.info_table_item > td.table_td")
                        vacancy_address = safe_get_text(core_area_2[0].get_text(strip=True))
                        vacancy_address_list.append(vacancy_address)
                        vacancy_size = safe_get_text(core_area_2[2].get_text(strip=True).split("/")[1].split("ã¡")[0])
                        vacancy_size_list.append(vacancy_size)
                        vacancy_floor = safe_get_text(core_area_2[3].get_text(strip=True).split("/")[0] + " Floor")
                        vacancy_floor_list.append(vacancy_floor)

                        print(f"{click_count} ë²ˆì§¸ ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ")
                        print(f"ğŸ“ ì£¼ì†Œ: {vacancy_address}")
                        print(f"ğŸ“ ë©´ì : {vacancy_size}í‰")
                        print(f"ğŸ¢ ì¸µìˆ˜: {vacancy_floor}")
                        print(f"ğŸ’° ë³´ì¦ê¸ˆ: {depositPrice}, ì›”ì„¸: {monthlyPrice}")
                        time.sleep(3)

                except ElementClickInterceptedException:
                    print(f"{n} ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì‹¤íŒ¨: ë‹¤ë¥¸ ìš”ì†Œì— ê°€ë ¤ì ¸ ìˆìŒ")
                except StaleElementReferenceException:
                    print(f"{n} ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì‹¤íŒ¨: ìš”ì†Œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ (stale)")
                except Exception as e:
                    print(f"{n} ë²ˆì§¸ ë§¤ë¬¼ í´ë¦­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜ˆì™¸ ë°œìƒ: {e}")

    # Create DataFrame from collected data (ìˆ˜ì§‘ ë°ì´í„° í”„ë ˆì„ ìƒì„±)
    final_vacancyBusan = pd.DataFrame({
        "Address": vacancy_address_list,
        "Size": vacancy_size_list,
        "Floor": vacancy_floor_list,
        "Month Price": monthlyPrice_list,
        "Depossit Price": depositPrice_list
    })

    final_vacancyBusan  # Display collected data (ìˆ˜ì§‘ëœ ë°ì´í„° í‘œì‹œ)

    # Save final result to CSV (ìµœì¢… ê²°ê³¼ CSV ì €ì¥)
    final_vacancyBusan.to_csv("../useData/finishPrepro/vacancy_location.csv", encoding="utf-8-sig")
    ''')
    st.markdown("<span style='color:orange; font-weight:bold; font-size:20px;'>Data Preprocessing (ë°ì´í„° ê°€ê³µ)</span>", unsafe_allow_html=True)
    st.code('''
    # Author: DongWhee Kim
    # Date: 2025-04-20
    # Description: Visualize processed warehouse vacancy data on an interactive map using Folium. (ê°€ê³µëœ ê³µì‹¤ ë°ì´í„°ë¥¼ folium ê¸°ë°˜ì˜ ì¸í„°ë™í‹°ë¸Œ ì§€ë„ì— ì‹œê°í™”)

    import warnings  # Suppress warning messages (ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥ ë°©ì§€)
    warnings.simplefilter(action='ignore', category=FutureWarning)

    import pandas as pd  # Data manipulation (ë°ì´í„° ì¡°ì‘ì„ ìœ„í•œ pandas ì„í¬íŠ¸)
    import re  # Regular expressions (ì •ê·œí‘œí˜„ì‹)
    from geopy.geocoders import Nominatim  # Geocoding API (ì£¼ì†Œ â†’ ìœ„ë„/ê²½ë„ ë³€í™˜)
    from tqdm import tqdm  # Progress bar for loops (ë£¨í”„ ì§„í–‰ í‘œì‹œ)
    import os  # Operating system interactions (í™˜ê²½ë³€ìˆ˜ ì ‘ê·¼ ë“± ì‹œìŠ¤í…œ ì—°ë™)
    import json  # JSON parsing (JSON íŒŒì‹±)
    import requests  # HTTP requests (HTTP ìš”ì²­)
    from dotenv import load_dotenv  # Load environment variables (.env í™˜ê²½ë³€ìˆ˜ ë¡œë“œ)
    import folium  # Map rendering (ì§€ë„ ì‹œê°í™”)
    from folium.features import CustomIcon  # Custom icon for map markers (ì§€ë„ ë§ˆì»¤ ì»¤ìŠ¤í…€ ì•„ì´ì½˜)
    import plotly.express as px  # Interactive charting (ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸ìš© plotly)

    # Load vacancy data from preprocessing output (ì „ì²˜ë¦¬ëœ ê³µì‹¤ ë°ì´í„° ë¡œë“œ)
    vacancyData = pd.read_csv("../useData/finishPrepro/vacancy_location.csv")
    vacancyData["Address"].value_counts()
    vacancyData = vacancyData.iloc[:,1:]  # Remove unnecessary index column (ë¶ˆí•„ìš”í•œ ì¸ë±ìŠ¤ ì œê±°)

    vacancyData.head()

    # Convert price columns to integers (ê°€ê²© ì»¬ëŸ¼ ì •ìˆ˜í˜• ë³€í™˜)
    vacancyData["Month Price"] = vacancyData["Month Price"].astype("int64")
    processed_DepositPrice = [
        v.replace(",", "").replace("ì–µ", "00000000")
        for v in vacancyData["Depossit Price"]
    ]
    vacancyData["Depossit Price"] = processed_DepositPrice
    vacancyData["Depossit Price"] = vacancyData["Depossit Price"].astype("int64")

    # Adjust units to KRW (ë‹¨ìœ„ ë³´ì •: ì²œì› â†’ ì›)
    vacancyData["Depossit Price"] = vacancyData["Depossit Price"] * 1000
    vacancyData["Month Price"] = vacancyData["Month Price"] * 1000
    vacancyData.iloc[3,-1] = vacancyData.iloc[3,-1]/1000  # íŠ¹ì • í–‰ì˜ ê°’ ìˆ˜ë™ ë³´ì •

    # Initialize lists to store geolocation (ìœ„ë„/ê²½ë„ ì €ì¥ ë¦¬ìŠ¤íŠ¸)
    lat_list = list()
    lng_list = list()

    # Load Google API Key from environment variables (.env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë”©)
    load_dotenv()
    myGoogleAPI = os.environ.get("googleAPI")

    # Geocode each address (ì£¼ì†Œ â†’ ìœ„ë„/ê²½ë„ ë³€í™˜)
    for v in vacancyData["Address"]:
        url = f'https://maps.googleapis.com/maps/api/geocode/json?address={v}&key={myGoogleAPI}'
        response = json.loads(requests.post(url).text)
        print(response)

        location = response["results"][0]['geometry']['location']
        lat = round(location['lat'], 2)
        lat_list.append(lat)
        lng = round(location['lng'], 2)
        lng_list.append(lng)

    print(len(lat_list))
    print(len(lng_list))

    # Store geolocation back to DataFrame (ìœ„ê²½ë„ ë°ì´í„° í”„ë ˆì„ì— ì €ì¥)
    vacancyData["Latitude"] = lat_list
    vacancyData["Longitude"] = lng_list
    vacancyData

    # Save with coordinates (ìœ„ê²½ë„ í¬í•¨ ë°ì´í„° ì €ì¥)
    vacancyData.to_csv("../useData/finishPrepro/vacancy_locationLaLo.csv", encoding="utf-8-sig")

    # Add new column to distinguish floor type (ì¸µìˆ˜ ìœ í˜• êµ¬ë¶„ ì»¬ëŸ¼ ì¶”ê°€)
    vacancyData["FloorType"] = ["ì§€í•˜" if "B" in v else "ì§€ìƒ" for v in vacancyData["Floor"]]

    # Remove ' Floor' and 'B' from floor string (ì¸µìˆ˜ ë¬¸ìì—´ ì •ì œ)
    def deleteFloor(x):
        x = x.replace(" Floor", "").replace("B", "")
        return x

    vacancyData["Floor"] = vacancyData["Floor"].apply(deleteFloor)
    vacancyData["Floor"] = vacancyData["Floor"].astype("int64")
    vacancyData.head(5)

    vacancyData.columns

    # Rearranging columns for clarity (ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬)
    renameColumns = ['FloorType', 'Address', 'Size', 'Floor', 'Month Price', 'Depossit Price', 'Latitude', 'Longitude']
    vacancyData = vacancyData[renameColumns]
    vacancyData.head()

    vacancyData.info()

    # Count listings by FloorType per address (ì£¼ì†Œë³„ ì§€ìƒ/ì§€í•˜ ê±´ìˆ˜ ì§‘ê³„)
    cntType = vacancyData.groupby(["Address", "FloorType"])["FloorType"].count()

    # Convert to DataFrame (ì§‘ê³„ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜)
    group_countType = pd.DataFrame(
        {
            "Address": [v[0] for v in cntType.index],
            "FloorType": [v[1] for v in cntType.index],
            "Type": cntType.values
        }
    )

    group_countType

    # Group and calculate average price per address (ì£¼ì†Œë³„ í‰ê·  ê°€ê²© ê³„ì‚°)
    groupPriceMean = vacancyData.groupby(["Address"])[["Month Price", "Depossit Price"]].mean()

    groupPriceMeanResult = pd.DataFrame(
        {
            "Address": groupPriceMean.index,
            "Month Price(AVG)": groupPriceMean["Month Price"].astype(int),
            "Depossit Price(AVG)": groupPriceMean["Depossit Price"].astype(int)
        }
    ).reset_index(drop=True)

    groupPriceMeanResult

    # Group and calculate average size (ì£¼ì†Œë³„ í‰ê·  ë©´ì  ê³„ì‚°)
    groupSizeMean = vacancyData.groupby(["Address"])["Size"].mean()
    groupSizeMeanResult = pd.DataFrame(
        {
            "Address": groupSizeMean.index,
            "Size(AVG ã¡)": groupSizeMean.values.astype(int)
        }
    )

    groupSizeMeanResult

    # Group and calculate average floor (ì£¼ì†Œë³„ í‰ê·  ì¸µìˆ˜ ê³„ì‚°)
    groupFloorMean = vacancyData.groupby(["Address"])["Floor"].mean()
    groupFloorMeanResult = pd.DataFrame(
        {
            "Address": groupFloorMean.index,
            "Floor(AVG)": groupFloorMean.values.astype(int)
        }
    )

    groupFloorMeanResult

    # Merge all group results into one DataFrame (ëª¨ë“  ìš”ì•½ ë°ì´í„° ë³‘í•©)
    groupMerge = (
        groupPriceMeanResult
        .merge(groupFloorMean, on="Address")
        .merge(groupSizeMean, on="Address")
    )
    groupMerge

    # Create base folium map centered on Busan (ë¶€ì‚° ì¤‘ì‹¬ ì¢Œí‘œ ê¸°ì¤€ ë² ì´ìŠ¤ ë§µ ìƒì„±)
    targetArea = folium.Map(
        location=[35.125135443661655, 128.96211911293543],
        zoom_start=11,
        tiles="CartoDB positron"
    )

    # Load GeoJSON data for Busan boundary (ë¶€ì‚°ì‹œ ê²½ê³„ GeoJSON ë¡œë”©)
    busanGeo = "../useData/koreaBusan.geojson"
    with open(busanGeo, encoding="utf-8-sig") as f:
        myGeo = json.load(f)

    # Style function for GeoJSON rendering (GeoJSON ìŠ¤íƒ€ì¼ í•¨ìˆ˜ ì •ì˜)
    def myGeo_style(x):
        return {
            "fillColor": "#dae080",
            "color": "",
            "weight": 0.5,
            "fillOpacity": 0.2
        }

    # Add boundary overlay to map (ì§€ë„ì— ê²½ê³„ì„  ì˜¤ë²„ë ˆì´ ì¶”ê°€)
    folium.GeoJson(
        data=myGeo,
        name="Busan Metropolitan City, Republic of Korea",
        style_function=myGeo_style
    ).add_to(targetArea)

    # Add each vacancy marker to map (ê° ê³µì‹¤ì— ëŒ€í•œ ë§ˆì»¤ ì¶”ê°€)
    for i, v in vacancyData.iterrows():
        lat = v["Latitude"]
        lon = v["Longitude"]
        address = v["Address"]

        if (address in groupMerge["Address"].values) and (address in group_countType["Address"].values):
            avg_floor = int(groupMerge[groupMerge["Address"] == address]["Floor"].values[0])
            avg_size = int(groupMerge[groupMerge["Address"] == address]["Size"].values[0])
            avg_month = groupMerge[groupMerge["Address"] == address]["Month Price(AVG)"].values[0]
            avg_deposit = groupMerge[groupMerge["Address"] == address]["Depossit Price(AVG)"].values[0]

            formatted_month = f"{avg_month:,}"
            formatted_deposit = f"{avg_deposit:,}"
            formatted_size = f"{avg_size:,}"

            floor_info_rows = group_countType[group_countType["Address"] == address]
            floor_type_strs = [
                f"{row['FloorType']} (count: {row['Type']})"
                for _, row in floor_info_rows.iterrows()
            ]
            floor_type_combined = ", ".join(floor_type_strs)

            noticeInfo = (
                f"<div style='background-color:#f9f9f9; padding:10px; border-radius:8px; font-size:13px; line-height:1.6'>"
                f"<b>(1) Area Name</b>: {address}<br>"
                f"<b>(2) Vacancy Type</b>: {floor_type_combined}<br>"
                f"<b>(3) Average Floor</b>: {avg_floor}<br>"
                f"<b>(4) Average Size</b>: {formatted_size}ã¡<br>"
                f"<b>(5) Average Monthly Price (KRW)</b>: {formatted_month}<br>"
                f"<b>(6) Average Monthly Deposit (KRW)</b>: {formatted_deposit}"
                f"</div>"
            )

            popup = folium.Popup(
                noticeInfo,
                max_width=400,
                min_width=30,
                max_height=300,
                show=True
            )

            myIcon = "../useData/myImage/vacancyMan.png"
            myIcon_edit = CustomIcon(
                icon_image=myIcon,
                icon_size=(31, 31),
                icon_anchor=(15, 15)
            )

            folium.Marker(
                location=[lat, lon],
                icon=myIcon_edit
            ).add_child(popup).add_to(targetArea)

        folium.CircleMarker(
            location=[lat, lon],
            radius=60,
            fill=True,
            fill_color="#e7ede3",
            color="#ee928c",
            fill_opacity=0.1
        ).add_to(targetArea)

    # Final interactive map result (ìµœì¢… ì§€ë„ ê²°ê³¼)
    targetArea
    ''')
